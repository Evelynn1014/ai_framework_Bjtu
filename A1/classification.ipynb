{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 导包\n",
    "* pandas用于快速读取数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e10d3b12a2b36bf0"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:44:39.770330Z",
     "start_time": "2024-04-13T02:44:39.763489Z"
    }
   },
   "id": "5d88def8ceb7e14a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 需要使用的工具类函数\n",
    "- 分批加载数据\n",
    "- 保存模型\n",
    "- 加载模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df2868f9d39d5f82"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def dataloader(X, y, BATCH_SIZE):\n",
    "    n = len(X)\n",
    "    for t in range(0, n, BATCH_SIZE):\n",
    "        yield X[t:t+BATCH_SIZE, ...], y[t:t+BATCH_SIZE, ...]\n",
    "        \n",
    "def save_params_to_file(model):\n",
    "    # Make save_weights/ accessible from every folders.\n",
    "    terminal_path = [\"src/slow/save_weights/\", \"slow/save_weights/\", \"save_weights/\", \"../save_weights/\"]\n",
    "    dirPath = None\n",
    "    for path in terminal_path:\n",
    "        if os.path.isdir(path):\n",
    "            dirPath = path\n",
    "    if dirPath == None:\n",
    "        raise FileNotFoundError(\"save_params_to_file(): Impossible to find save_weights/ from current folder. You need to manually add the path to it in the \\'terminal_path\\' list and the run the function again.\")\n",
    "\n",
    "    weights = model.get_params()\n",
    "    with open(dirPath + \"final_weights.pkl\",\"wb\") as f:\n",
    "\t    pickle.dump(weights, f)\n",
    "        \n",
    "def load_params_from_file(model):\n",
    "    terminal_path = [\"src/slow/save_weights/final_weights.pkl\", \"slow/save_weights/final_weights.pkl\",\n",
    "    \"save_weights/final_weights.pkl\", \"../save_weights/final_weights.pkl\"]\n",
    "\n",
    "    filePath = None\n",
    "    for path in terminal_path:\n",
    "        if os.path.isfile(path):\n",
    "            filePath = path\n",
    "    if filePath == None:\n",
    "        raise FileNotFoundError('load_params_from_file(): Cannot find final_weights.pkl from your current folder. You need to manually add it to terminal_path list and the run the function again.')\n",
    "\n",
    "    pickle_in = open(filePath, 'rb')\n",
    "    params = pickle.load(pickle_in)\n",
    "    model.set_params(params)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:44:39.774777Z",
     "start_time": "2024-04-13T02:44:39.766303Z"
    }
   },
   "id": "fc0857ad34d59bb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers\n",
    "- 定义Fc层\n",
    "- 定义Loss函数：CELoss\n",
    "- 定义Softmax\n",
    "- 定义TanH/Relu\n",
    "- 定义Adam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ed3b7108bd85335"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class Fc():\n",
    "    def __init__(self, row, column):\n",
    "        self.row = row\n",
    "        self.col = column\n",
    "        \n",
    "        #Initialize Weight/bias.\n",
    "        bound = 1 / np.sqrt(self.row)\n",
    "        self.W = {'val': np.random.uniform(low=-bound, high=bound, size=(self.row, self.col)), 'grad': 0}\n",
    "        self.b = {'val': np.random.uniform(low=-bound, high=bound, size=(1, self.row)), 'grad': 0}\n",
    "        \n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, fc):\n",
    "        self.cache = fc\n",
    "        A_fc = np.dot(fc, self.W['val'].T) + self.b['val']\n",
    "        return A_fc\n",
    "\n",
    "    def backward(self, deltaL):\n",
    "        fc = self.cache\n",
    "        m = fc.shape[0]\n",
    "\n",
    "        #Compute gradient.\n",
    "    \n",
    "        self.W['grad'] = (1/m) * np.dot(deltaL.T, fc)\n",
    "        self.b['grad'] = (1/m) * np.sum(deltaL, axis = 0)\n",
    "\n",
    "        #Compute error.\n",
    "        new_deltaL = np.dot(deltaL, self.W['val']) \n",
    "        #We still need to multiply new_deltaL by the derivative of the activation\n",
    "        #function which is done in TanH.backward().\n",
    "\n",
    "        return new_deltaL, self.W['grad'], self.b['grad']\n",
    "\n",
    "class AdamGD():\n",
    "\n",
    "    def __init__(self, lr, beta1, beta2, epsilon, params):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.params = params\n",
    "        \n",
    "        self.momentum = {}\n",
    "        self.rmsprop = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.momentum['vd' + key] = np.zeros(self.params[key].shape)\n",
    "            self.rmsprop['sd' + key] = np.zeros(self.params[key].shape)\n",
    "\n",
    "    def update_params(self, grads):\n",
    "\n",
    "        for key in self.params:\n",
    "            # Momentum update.\n",
    "            self.momentum['vd' + key] = (self.beta1 * self.momentum['vd' + key]) + (1 - self.beta1) * grads['d' + key] \n",
    "            # RMSprop update.\n",
    "            self.rmsprop['sd' + key] =  (self.beta2 * self.rmsprop['sd' + key]) + (1 - self.beta2) * grads['d' + key]**2 \n",
    "            # Update parameters.\n",
    "            self.params[key] += -self.lr * self.momentum['vd' + key] / (np.sqrt(self.rmsprop['sd' + key]) + self.epsilon)  \n",
    "\n",
    "        return self.params\n",
    "\n",
    "class TanH():\n",
    " \n",
    "    def __init__(self, alpha = 1.7159):\n",
    "        self.alpha = alpha\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return self.alpha * np.tanh(X)\n",
    "\n",
    "    def backward(self, new_deltaL):\n",
    "        X = self.cache\n",
    "        return new_deltaL * (1 - np.tanh(X)**2)\n",
    "\n",
    "\n",
    "class Softmax():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=1)[:, np.newaxis]\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get(self, y_pred, y):\n",
    "        batch_size = y_pred.shape[1]\n",
    "        deltaL = y_pred - y\n",
    "        loss = -np.sum(y * np.log(y_pred)) / batch_size\n",
    "        return loss, deltaL\n",
    "    \n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, new_deltaL):\n",
    "        X = self.cache\n",
    "        dX = np.where(X > 0, 1, 0)  # Derivative of ReLU\n",
    "        return new_deltaL * dX"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:44:39.800452Z",
     "start_time": "2024-04-13T02:44:39.789694Z"
    }
   },
   "id": "319cde824a042c35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "- Implement MLP Classifier\n",
    "- Softmax Output\n",
    "- Hidden layers depth and number of neurons are flexible\n",
    "- Activation select TanH"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8c0eadaeddeeb1e"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class Net5:\n",
    "    def __init__(self, input=28 * 28, hidden1=14 * 28, hidden2=14 * 14, output=10):\n",
    "        self.fc1 = Fc(row = hidden1, column = input)\n",
    "        self.tanh3 = TanH()\n",
    "        self.fc2 = Fc(row = hidden2, column = hidden1)\n",
    "        self.tanh4 = TanH()\n",
    "        self.fc3 = Fc(row = output, column = hidden2)\n",
    "        self.softmax = Softmax()\n",
    "        self.layers = [self.fc1, self.fc2, self.fc3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        fc1 = self.fc1.forward(X) \n",
    "        act3 = self.tanh3.forward(fc1)\n",
    "        fc2 = self.fc2.forward(act3)\n",
    "        act4 = self.tanh4.forward(fc2)\n",
    "        fc3 = self.fc3.forward(act4)\n",
    "        softmax_pred = self.softmax.forward(fc3)\n",
    "        return softmax_pred\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        #Compute gradient for weight/bias between fc3 and fc2.\n",
    "        deltaL, dW3, db3, = self.fc3.backward(deltaL)\n",
    "\n",
    "        #Compute error at fc2 layer.\n",
    "        deltaL = self.tanh4.backward(deltaL) #(1x84) \n",
    "        \n",
    "        #Compute gradient for weight/bias between fc2 and fc1.\n",
    "        deltaL, dW2, db2 = self.fc2.backward(deltaL)\n",
    "        #Compute error at fc1 layer.\n",
    "        deltaL = self.tanh3.backward(deltaL) #(1x120)\n",
    "        \n",
    "        deltaL, dW1, db1 = self.fc1.backward(deltaL) #(1x400)\n",
    "\n",
    "        grads = { \n",
    "                'dW1': dW1, 'db1': db1,\n",
    "                'dW2': dW2, 'db2': db2,\n",
    "                'dW3': dW3, 'db3': db3,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W['val']\n",
    "            params['b' + str(i+1)] = layer.b['val']\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W['val'] = params['W'+ str(i+1)]\n",
    "            layer.b['val'] = params['b' + str(i+1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:44:39.818828Z",
     "start_time": "2024-04-13T02:44:39.806961Z"
    }
   },
   "id": "2754f3613492090f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d641713b7325cc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"data/mnist_train.csv\"\n",
    "df = pd.read_csv(data_path, header=1)\n",
    "dataset = np.array(df) # 60000 * (28 * 28 + 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3c056a1cb232ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Divide the data set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6dea4d38d405ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = dataset[:,:1] # (59999, 0)\n",
    "features = dataset[:,1:] # (59999, 28 * 28)\n",
    "\n",
    "size = 0.8\n",
    "train_size = int(0.8*len(features))\n",
    "X = features[:train_size] # (404, 10)\n",
    "X_test = features[train_size:] # (102, 10)\n",
    "y = labels[:train_size]\n",
    "y_test = labels[train_size:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e50a680838f7a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322b9caf94111038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征归一化\n",
    "X, X_test = X/float(255), X_test/float(255)\n",
    "X -= np.mean(X)\n",
    "X_test -= np.mean(X_test)\n",
    "\n",
    "# print(\"Train and Validation set split: OK\\n\")\n",
    "X_train = X\n",
    "y_train = y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5b6f06fe71ebb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Loss Optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "887478ae5c9a3e79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Net5()\n",
    "\n",
    "# CE for classification (with softmax)\n",
    "cost = CrossEntropyLoss()\n",
    "\n",
    "params = model.get_params()\n",
    "optimizer = AdamGD(lr = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, params = model.get_params())    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10da61bf93dabb36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train\n",
    "使用Accuray来评估Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6de66d7faf1ccc0a"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa906932012b2049",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:45:06.463796Z",
     "start_time": "2024-04-13T02:44:39.843090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------TRAINING-----------------\n",
      "\n",
      "EPOCHS: 1\n",
      "BATCH_SIZE: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1: 100%|██████████| 479/479 [00:25<00:00, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-loss: 2323.419538 | train-acc: 0.994\n"
     ]
    }
   ],
   "source": [
    "train_costs, val_costs = [], []\n",
    "print(\"----------------TRAINING-----------------\\n\")\n",
    "\n",
    "NB_EPOCH = 1\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(\"EPOCHS: {}\".format(NB_EPOCH))\n",
    "print(\"BATCH_SIZE: {}\".format(BATCH_SIZE))\n",
    "print(\"\")\n",
    "\n",
    "nb_train_examples = len(X_train)\n",
    "# nb_val_examples = len(X_val)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc = 0 \n",
    "\n",
    "    pbar = trange(nb_train_examples // BATCH_SIZE)\n",
    "    train_loader = dataloader(X_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    for i, (X_batch, y_batch) in zip(pbar, train_loader):\n",
    "        y_pred = model.forward(X_batch)\n",
    "        loss, deltaL = cost.get(y_pred, y_batch)\n",
    "\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "    \n",
    "        # print(loss)\n",
    "        train_loss += loss * BATCH_SIZE\n",
    "        \n",
    "        # Accuracy\n",
    "        train_acc += sum((np.argmax(y_batch, axis=1) == np.argmax(y_pred, axis=1)))\n",
    "\n",
    "        pbar.set_description(\"[Train] Epoch {}\".format(epoch+1))\n",
    "\n",
    "    train_loss /= nb_train_examples\n",
    "    train_costs.append(train_loss)\n",
    "    \n",
    "    # Accuracy\n",
    "    train_acc /= nb_train_examples\n",
    "\n",
    "    info_train = \"train-loss: {:0.6f} | train-acc: {:0.3f}\"\n",
    "    print(info_train.format(train_loss, train_acc))\n",
    "    save_params_to_file(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "使用Accuray来评估Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "992add40e36883f8"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba4a118727cf6df1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T02:45:09.861389Z",
     "start_time": "2024-04-13T02:45:06.452936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------LOAD PRETRAINED MODEL--------------\n",
      "\n",
      "Load pretrained model: OK\n",
      "\n",
      "--------------------EVALUATION-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 99/99 [00:03<00:00, 31.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-loss: 2288.292239 | test-acc: 0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data_path = \"data/mnist_test.csv\"\n",
    "df = pd.read_csv(data_path, header=1)\n",
    "dataset = np.array(df) # N * (28 * 28 + 1)\n",
    "\n",
    "# Divide the data set\n",
    "y_test = dataset[:,:1] # (n, 0)\n",
    "X_test = dataset[:,1:] # (n, 28 * 28)\n",
    "\n",
    "# 特征归一化\n",
    "X_test = X_test/float(255)\n",
    "X_test -= np.mean(X_test)\n",
    "\n",
    "print(\"\\n--------------LOAD PRETRAINED MODEL--------------\\n\")\n",
    "cost = CrossEntropyLoss()\n",
    "model = Net5()\n",
    "model = load_params_from_file(model)\n",
    "print(\"Load pretrained model: OK\\n\")\n",
    "\n",
    "print(\"--------------------EVALUATION-------------------\\n\")\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "nb_test_examples = len(X_test)\n",
    "test_loss = 0\n",
    "\n",
    "# RMSE and MAPE\n",
    "test_acc = 0\n",
    "\n",
    "pbar = trange(nb_test_examples // BATCH_SIZE)\n",
    "test_loader = dataloader(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "for i, (X_batch, y_batch) in zip(pbar, test_loader):\n",
    "  \n",
    "    y_pred = model.forward(X_batch)\n",
    "    loss, deltaL = cost.get(y_pred, y_batch)\n",
    "\n",
    "    test_loss += loss * BATCH_SIZE\n",
    "    \n",
    "   # Accuracy\n",
    "    test_acc += sum((np.argmax(y_batch, axis=1) == np.argmax(y_pred, axis=1)))\n",
    "\n",
    "    pbar.set_description(\"Evaluation\")\n",
    "\n",
    "test_loss /= nb_test_examples\n",
    "\n",
    "# RMSE and MAPE\n",
    "# Accuracy\n",
    "test_acc /= nb_train_examples\n",
    "\n",
    "info_test = \"test-loss: {:0.6f} | test-acc: {:0.3f}\"\n",
    "print(info_test.format(test_loss, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
