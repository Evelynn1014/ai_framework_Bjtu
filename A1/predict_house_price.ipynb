{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 导包\n",
    "- pandas用于快速读取数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f17508bc0bfd5b1a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T05:08:01.042457Z",
     "start_time": "2024-04-13T05:08:01.026321Z"
    }
   },
   "id": "9b0be5963e1048a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 需要使用的工具类函数\n",
    "- 分批加载数据\n",
    "- 保存模型\n",
    "- 加载模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ff7beb8d658364b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def dataloader(X, y, BATCH_SIZE):\n",
    "    n = len(X)\n",
    "    for t in range(0, n, BATCH_SIZE):\n",
    "        yield X[t:t+BATCH_SIZE, ...], y[t:t+BATCH_SIZE, ...]\n",
    "        \n",
    "def save_params_to_file(model):\n",
    "    terminal_path = [\"src/slow/save_weights/\", \"slow/save_weights/\", \"save_weights/\", \"../save_weights/\"]\n",
    "    dirPath = None\n",
    "    for path in terminal_path:\n",
    "        if os.path.isdir(path):\n",
    "            dirPath = path\n",
    "    if dirPath == None:\n",
    "        raise FileNotFoundError(\"save_params_to_file(): Impossible to find save_weights/ from current folder. You need to manually add the path to it in the \\'terminal_path\\' list and the run the function again.\")\n",
    "\n",
    "    weights = model.get_params()\n",
    "    with open(dirPath + \"final_weights.pkl\",\"wb\") as f:\n",
    "\t    pickle.dump(weights, f)\n",
    "        \n",
    "def load_params_from_file(model):\n",
    "    terminal_path = [\"src/slow/save_weights/final_weights.pkl\", \"slow/save_weights/final_weights.pkl\",\n",
    "    \"save_weights/final_weights.pkl\", \"../save_weights/final_weights.pkl\"]\n",
    "\n",
    "    filePath = None\n",
    "    for path in terminal_path:\n",
    "        if os.path.isfile(path):\n",
    "            filePath = path\n",
    "    if filePath == None:\n",
    "        raise FileNotFoundError('load_params_from_file(): Cannot find final_weights.pkl from your current folder. You need to manually add it to terminal_path list and the run the function again.')\n",
    "\n",
    "    pickle_in = open(filePath, 'rb')\n",
    "    params = pickle.load(pickle_in)\n",
    "    model.set_params(params)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T05:08:01.051684Z",
     "start_time": "2024-04-13T05:08:01.038765Z"
    }
   },
   "id": "d0a58855c0d47835"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers\n",
    "- 定义Fc层\n",
    "- 定义Loss函数：MSELoss\n",
    "- 定义Softmax\n",
    "- 定义TanH/Relu\n",
    "- 定义Adam"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a13dce58b327252e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Fc():\n",
    "\n",
    "    def __init__(self, row, column):\n",
    "        self.row = row\n",
    "        self.col = column\n",
    "        \n",
    "        #Initialize Weight/bias.\n",
    "        bound = 1 / np.sqrt(self.row)\n",
    "        self.W = {'val': np.random.uniform(low=-bound, high=bound, size=(self.row, self.col)), 'grad': 0}\n",
    "        self.b = {'val': np.random.uniform(low=-bound, high=bound, size=(1, self.row)), 'grad': 0}\n",
    "        \n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, fc):\n",
    "        self.cache = fc\n",
    "        A_fc = np.dot(fc, self.W['val'].T) + self.b['val']\n",
    "        return A_fc\n",
    "\n",
    "    def backward(self, deltaL):\n",
    "        fc = self.cache\n",
    "        m = fc.shape[0]\n",
    "\n",
    "        #Compute gradient.\n",
    "    \n",
    "        self.W['grad'] = (1/m) * np.dot(deltaL.T, fc)\n",
    "        self.b['grad'] = (1/m) * np.sum(deltaL, axis = 0)\n",
    "\n",
    "        #Compute error.\n",
    "        new_deltaL = np.dot(deltaL, self.W['val']) \n",
    "        #We still need to multiply new_deltaL by the derivative of the activation\n",
    "        #function which is done in TanH.backward().\n",
    "\n",
    "        return new_deltaL, self.W['grad'], self.b['grad']\n",
    "\n",
    "class AdamGD():\n",
    "\n",
    "    def __init__(self, lr, beta1, beta2, epsilon, params):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.params = params\n",
    "        \n",
    "        self.momentum = {}\n",
    "        self.rmsprop = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.momentum['vd' + key] = np.zeros(self.params[key].shape)\n",
    "            self.rmsprop['sd' + key] = np.zeros(self.params[key].shape)\n",
    "\n",
    "    def update_params(self, grads):\n",
    "\n",
    "        for key in self.params:\n",
    "            # Momentum update.\n",
    "            self.momentum['vd' + key] = (self.beta1 * self.momentum['vd' + key]) + (1 - self.beta1) * grads['d' + key] \n",
    "            # RMSprop update.\n",
    "            self.rmsprop['sd' + key] =  (self.beta2 * self.rmsprop['sd' + key]) + (1 - self.beta2) * grads['d' + key]**2 \n",
    "            # Update parameters.\n",
    "            self.params[key] += -self.lr * self.momentum['vd' + key] / (np.sqrt(self.rmsprop['sd' + key]) + self.epsilon)  \n",
    "\n",
    "        return self.params\n",
    "\n",
    "class TanH():\n",
    " \n",
    "    def __init__(self, alpha = 1.7159):\n",
    "        self.alpha = alpha\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return self.alpha * np.tanh(X)\n",
    "\n",
    "    def backward(self, new_deltaL):\n",
    "        X = self.cache\n",
    "        return new_deltaL * (1 - np.tanh(X)**2)\n",
    "\n",
    "\n",
    "class Softmax():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.exp(X) / np.sum(np.exp(X), axis=1)[:, np.newaxis]\n",
    "\n",
    "class MSELoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, y_pred, y):\n",
    "        batch_size = y_pred.shape[1]\n",
    "        deltaL = y_pred - y\n",
    "        loss = np.sum(np.square(y - y_pred)) / batch_size\n",
    "        return loss, deltaL\n",
    "\n",
    "    \n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, new_deltaL):\n",
    "        X = self.cache\n",
    "        dX = np.where(X > 0, 1, 0)  # Derivative of ReLU\n",
    "        return new_deltaL * dX"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T05:08:01.085778Z",
     "start_time": "2024-04-13T05:08:01.041635Z"
    }
   },
   "id": "89b8796fe66c0626"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "- Implement MLP Regression\n",
    "- Hidden layers depth and number of neurons are flexible\n",
    "- Activation select TanH"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38ccb77362815c00"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Net5:\n",
    "    def __init__(self, input=10, hidden1=120, hidden2=84, output=1):\n",
    "        self.fc1 = Fc(row = hidden1, column = input)\n",
    "        self.tanh3 = TanH()\n",
    "        self.fc2 = Fc(row = hidden2, column = hidden1)\n",
    "        self.tanh4 = TanH()\n",
    "        self.fc3 = Fc(row = output, column = hidden2)\n",
    "        # self.softmax = Softmax()\n",
    "        self.layers = [self.fc1, self.fc2, self.fc3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        fc1 = self.fc1.forward(X) \n",
    "        act3 = self.tanh3.forward(fc1)\n",
    "        fc2 = self.fc2.forward(act3)\n",
    "        act4 = self.tanh4.forward(fc2)\n",
    "        fc3 = self.fc3.forward(act4)\n",
    "        # y_pred = self.softmax.forward(fc3)\n",
    "        return fc3\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        #Compute gradient for weight/bias between fc3 and fc2.\n",
    "        deltaL, dW3, db3, = self.fc3.backward(deltaL)\n",
    "\n",
    "        #Compute error at fc2 layer.\n",
    "        deltaL = self.tanh4.backward(deltaL) \n",
    "        \n",
    "        #Compute gradient for weight/bias between fc2 and fc1.\n",
    "        deltaL, dW2, db2 = self.fc2.backward(deltaL)\n",
    "        #Compute error at fc1 layer.\n",
    "        deltaL = self.tanh3.backward(deltaL) \n",
    "        \n",
    "        deltaL, dW1, db1 = self.fc1.backward(deltaL)\n",
    "\n",
    "        grads = { \n",
    "                'dW1': dW1, 'db1': db1,\n",
    "                'dW2': dW2, 'db2': db2,\n",
    "                'dW3': dW3, 'db3': db3,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W['val']\n",
    "            params['b' + str(i+1)] = layer.b['val']\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W['val'] = params['W'+ str(i+1)]\n",
    "            layer.b['val'] = params['b' + str(i+1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T05:08:01.092164Z",
     "start_time": "2024-04-13T05:08:01.090875Z"
    }
   },
   "id": "38bb9543d15fc566"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1485fbe64e41188d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"data/Boston_house_prices.xlsx\"\n",
    "df = pd.read_excel(data_path, sheet_name = 0)\n",
    "dataset = np.array(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68055538baaa2b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Divide the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1afaae5683184597"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = dataset[:,-1:]\n",
    "features = dataset[:,:-1]\n",
    "\n",
    "size = 0.8\n",
    "train_size = int(0.8*len(features))\n",
    "\n",
    "X = features[:train_size] # (404, 10)\n",
    "X_test = features[train_size:] # (102, 10)\n",
    "y = labels[:train_size]\n",
    "y_test = labels[train_size:]\n",
    "\n",
    "X_train = X\n",
    "y_train = y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba6c791fe83f7c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Loss Optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38c46ffc856da16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Net5()\n",
    "\n",
    "# MSE for regression\n",
    "cost = MSELoss()\n",
    "\n",
    "params = model.get_params()\n",
    "optimizer = AdamGD(lr = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, params = model.get_params())   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54553097005387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and Evauation\n",
    "使用RMSE和MAPE来评估Train/Evaluate结果"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eaa196d002f6d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T05:08:01.512104Z",
     "start_time": "2024-04-13T05:08:01.100106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------TRAINING-----------------\n",
      "\n",
      "EPOCHS: 2\n",
      "BATCH_SIZE: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-loss: 47493.856179 | train-rmse: 0.207\n",
      "train-loss: 47493.856179 | train-mape: 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 55.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-loss: 18469.112809 | train-rmse: 0.130\n",
      "train-loss: 18469.112809 | train-mape: 0.006\n",
      "--------------------EVALUATION-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 102.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-loss: 8790.368155 | test-rmse: 0.023\n",
      "test-loss: 8790.368155 | test-mape: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_costs, val_costs = [], []\n",
    "\n",
    "print(\"----------------TRAINING-----------------\\n\")\n",
    "\n",
    "NB_EPOCH = 2\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(\"EPOCHS: {}\".format(NB_EPOCH))\n",
    "print(\"BATCH_SIZE: {}\".format(BATCH_SIZE))\n",
    "print(\"\")\n",
    "\n",
    "nb_train_examples = len(X_train)\n",
    "# nb_val_examples = len(X_val)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "\n",
    "    #-------------------------------------------------------------------------------\n",
    "    #                                       \n",
    "    #                               TRAINING PART\n",
    "    #\n",
    "    #-------------------------------------------------------------------------------\n",
    "\n",
    "    train_loss = 0\n",
    "    \n",
    "    # RMSE and MAPE\n",
    "    train_rmse = 0\n",
    "    train_mape = 0\n",
    "\n",
    "    pbar = trange(nb_train_examples // BATCH_SIZE)\n",
    "    train_loader = dataloader(X_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    for i, (X_batch, y_batch) in zip(pbar, train_loader):\n",
    "        y_pred = model.forward(X_batch)\n",
    "        loss, deltaL = cost.get(y_pred, y_batch)\n",
    "\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss * BATCH_SIZE\n",
    "        \n",
    "        # RMSE and MAPE\n",
    "        train_rmse += np.sqrt(np.mean((y_batch - y_pred) ** 2))\n",
    "        train_mape += np.mean(np.abs((y_batch - y_pred) / y_batch))\n",
    "        \n",
    "        pbar.set_description(\"[Train] Epoch {}\".format(epoch+1))\n",
    "\n",
    "    train_loss /= nb_train_examples\n",
    "    train_costs.append(train_loss)\n",
    "    \n",
    "    # RMSE and MAPE\n",
    "    train_rmse /= nb_train_examples\n",
    "    train_mape /= nb_train_examples\n",
    "    \n",
    "    info_train = \"train-loss: {:0.6f} | train-rmse: {:0.3f}\"\n",
    "    print(info_train.format(train_loss, train_rmse))\n",
    "    info_train = \"train-loss: {:0.6f} | train-mape: {:0.3f}\"\n",
    "    print(info_train.format(train_loss, train_mape))\n",
    "    save_params_to_file(model)\n",
    "    \n",
    "    \n",
    "print(\"--------------------EVALUATION-------------------\\n\")\n",
    "BATCH_SIZE = 100\n",
    "nb_test_examples = len(X_test)\n",
    "test_loss = 0\n",
    "\n",
    "# RMSE and MAPE\n",
    "test_rmse = 0\n",
    "test_mape = 0\n",
    "\n",
    "pbar = trange(nb_test_examples // BATCH_SIZE)\n",
    "test_loader = dataloader(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "for i, (X_batch, y_batch) in zip(pbar, test_loader):\n",
    "  \n",
    "    y_pred = model.forward(X_batch)\n",
    "    loss, deltaL = cost.get(y_pred, y_batch)\n",
    "\n",
    "    test_loss += loss * BATCH_SIZE\n",
    "    \n",
    "    # RMSE and MAPE\n",
    "    test_rmse += np.sqrt(np.mean((y_batch - y_pred) ** 2))\n",
    "    test_mape += np.mean(np.abs((y_batch - y_pred) / y_batch))\n",
    "\n",
    "    pbar.set_description(\"Evaluation\")\n",
    "\n",
    "test_loss /= nb_test_examples\n",
    "\n",
    "# RMSE and MAPE\n",
    "test_rmse /= nb_train_examples\n",
    "test_mape /= nb_train_examples\n",
    "\n",
    "info_test = \"test-loss: {:0.6f} | test-rmse: {:0.3f}\"\n",
    "print(info_test.format(test_loss, test_rmse))\n",
    "info_test = \"test-loss: {:0.6f} | test-mape: {:0.3f}\"\n",
    "print(info_test.format(test_loss, test_mape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
